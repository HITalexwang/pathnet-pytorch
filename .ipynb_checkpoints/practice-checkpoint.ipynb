{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BinaryClassificationTask():\n",
    "    def __init__(self):\n",
    "        self.classes = list(range(10))\n",
    "        trans = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "        self.train_set = dset.MNIST(root='./data/', train=True, transform=trans, download=False)\n",
    "        self.test_set = dset.MNIST(root='./data/', train=False, transform=trans, download=False)\n",
    "        \n",
    "    def salt_and_pepper_noise(self, prob, x):\n",
    "        probs = torch.rand(*x.size())\n",
    "        x[probs < prob / 2] = 0\n",
    "        x[probs > 1 - prob / 2] = 1\n",
    "        return x\n",
    "        \n",
    "    def init(self, batch_size, noise_prob):\n",
    "        labels = np.random.choice(self.classes, 2, replace=False)\n",
    "\n",
    "        # For the next task we won't encounter the same label again\n",
    "        self.classes = [x for x in self.classes if x not in labels] \n",
    "        \n",
    "        print('Binary classification between {} and {}'.format(labels[0], labels[1]))\n",
    "        \n",
    "        train_set = self.convert2tensor(self.train_set, labels, \n",
    "                                        train=True, noise_prob = noise_prob)\n",
    "        test_set = self.convert2tensor(self.test_set, labels)\n",
    "        \n",
    "        train_loader = DataLoader(train_set, \n",
    "                                  batch_size, \n",
    "                                  shuffle=True)\n",
    "        \n",
    "        test_loader = DataLoader(test_set, \n",
    "                                 batch_size, \n",
    "                                 shuffle=True)\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def convert2tensor(self, dset, labels, train=False, noise_prob=None):\n",
    "        x_set = []\n",
    "        y_set = []\n",
    "        \n",
    "        for x, y in dset:\n",
    "            if y == labels[0]:\n",
    "                x_set.append(x)\n",
    "                y_set.append(torch.LongTensor([0]))\n",
    "            elif y == labels[1]:\n",
    "                x_set.append(x)\n",
    "                y_set.append(torch.LongTensor([1]))\n",
    "        \n",
    "        x_set = torch.cat(x_set, 0)\n",
    "        x_set = x_set.view(x_set.size()[0], -1)\n",
    "        if train:\n",
    "            x_set = self.salt_and_pepper_noise(noise_prob, x_set)\n",
    "        \n",
    "        y_set = torch.cat(y_set, 0)\n",
    "        dataset = TensorDataset(x_set, y_set)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "L = 3  # Layers\n",
    "M = 10 # Modules\n",
    "N = 3  # Max modules in one layer\n",
    "P = 64 # Population\n",
    "G = 100 # Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_pathways():\n",
    "    layer_configs = list(itertools.combinations_with_replacement(list(range(M)), N))\n",
    "    layer_configs = np.array(layer_configs)\n",
    "    indices = np.random.choice(len(layer_configs), (P, L))\n",
    "    pathways = layer_configs[indices]\n",
    "        \n",
    "    return pathways # Shape: P x L x N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PathNet(nn.Module):\n",
    "    '''\n",
    "        The architecture follows the paper specifications\n",
    "            https://arxiv.org/pdf/1701.08734.pdf\n",
    "        for SUPERVISED LEARNING tasks\n",
    "    '''\n",
    "    def __init__(self, M, L=3):\n",
    "        super(PathNet, self).__init__()\n",
    "        self.M = M\n",
    "        self.L = L\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer1 = [nn.Linear(28 * 28, 20) for i in range(M)]\n",
    "        self.layer2 = [nn.Linear(20, 20) for i in range(M)]\n",
    "        self.layer3 = [nn.Linear(20, 20) for i in range(M)]\n",
    "        \n",
    "        self.optimizer_params = []\n",
    "        for m in range(self.M):\n",
    "            self.optimizer_params.append({'params' : self.layer1[m].parameters()})\n",
    "            self.optimizer_params.append({'params' : self.layer2[m].parameters()})\n",
    "            self.optimizer_params.append({'params' : self.layer3[m].parameters()})\n",
    "        \n",
    "    def sum_layer(self, layer_outputs):\n",
    "        if len(layer_outputs) == 1:\n",
    "            return layer_outputs[0]\n",
    "        \n",
    "        return [layer_outputs[i] + layer_outputs[i+1] \n",
    "                for i in range(len(layer_outputs) - 1)][0]\n",
    "    \n",
    "    def forward(self, x, pathway):\n",
    "        layer1_active_modules_index = list(set(pathway[0]))\n",
    "        layer2_active_modules_index = list(set(pathway[1]))\n",
    "        layer3_active_modules_index = list(set(pathway[2]))\n",
    "        \n",
    "        layer1_output = [self.relu(self.layer1[m](x)) for m in layer1_active_modules_index]\n",
    "        layer1_output_sum = self.sum_layer(layer1_output)\n",
    "\n",
    "        layer2_output = [self.relu(self.layer2[m](layer1_output_sum)) for m in layer2_active_modules_index]\n",
    "        layer2_output_sum = self.sum_layer(layer2_output)\n",
    "\n",
    "        layer3_output = [self.relu(self.layer3[m](layer2_output_sum)) for m in layer3_active_modules_index]\n",
    "        layer3_output_sum = self.sum_layer(layer3_output)\n",
    "\n",
    "        output = self.last_layer(layer3_output_sum)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def initialize_new_task(self, last_layer):\n",
    "        self.last_layer = last_layer\n",
    "        self.optimizer_params.append({'params' : last_layer.parameters()})\n",
    "        \n",
    "    def output_shape_calculator(self):\n",
    "        pass\n",
    "    \n",
    "    def get_optimizer_params(self):\n",
    "        return self.optimizer_params\n",
    "    \n",
    "    def done_task(self, best_pathway):\n",
    "        # Freeze best pathway\n",
    "        # Re-initialize all others\n",
    "        layer1_active_modules_index = list(set(best_pathway[0]))\n",
    "        layer2_active_modules_index = list(set(best_pathway[1]))\n",
    "        layer3_active_modules_index = list(set(best_pathway[2]))\n",
    "        \n",
    "        self.optimizer_params = []\n",
    "    \n",
    "        # Freeze and add parameters to train\n",
    "        for i in range(self.M):\n",
    "            if i in layer1_active_modules_index:\n",
    "                self.layer1[i].requires_grad = False\n",
    "            else:\n",
    "                self.layer1[i].reset_parameters()\n",
    "                self.layer1[i].requires_grad = True\n",
    "                self.optimizer_params.append({'params' : self.layer1[i].parameters()})\n",
    "        \n",
    "            if i in layer2_active_modules_index:\n",
    "                self.layer2[i].requires_grad = False\n",
    "            else:\n",
    "                self.layer2[i].reset_parameters()\n",
    "                self.layer2[i].requires_grad = True\n",
    "                self.optimizer_params.append({'params' : self.layer2[i].parameters()})\n",
    "        \n",
    "            if i in layer3_active_modules_index:\n",
    "                self.layer3[i].requires_grad = False\n",
    "            else:\n",
    "                self.layer3[i].reset_parameters()\n",
    "                self.layer3[i].requires_grad = True\n",
    "                self.optimizer_params.append({'params' : self.layer3[i].parameters()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryEvolutionTrainer(object):\n",
    "    def __init__(self, model, optimizer, loss_func, \n",
    "                 train_loader, test_loader, convergence_threshold,\n",
    "                 batch_epochs=50):\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.batch_epochs = batch_epochs\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.convergence_threshold = convergence_threshold\n",
    "        self.optimizer = optimizer\n",
    "      \n",
    "    def initialize_pathways():\n",
    "        layer_configs = list(itertools.combinations_with_replacement(list(range(M)), N))\n",
    "        layer_configs = np.array(layer_configs)\n",
    "        indices = np.random.choice(len(layer_configs), (P, L))\n",
    "        pathways = layer_configs[indices]\n",
    "\n",
    "        return pathways # Shape: P x L x N\n",
    "    \n",
    "    def mutate(self, pathway):\n",
    "        prob_mutate = 1./ (L * N) # Increase probability of mutation\n",
    "\n",
    "        # Probability of mutation for every element\n",
    "        prob = np.random.rand(L, N)\n",
    "\n",
    "        # Mutations for chosen elements\n",
    "        permutations = np.random.randint(-2, 2, size=(L, N))\n",
    "        permutations[prob > prob_mutate] = 0\n",
    "\n",
    "        # Mutate\n",
    "        pathway = (pathway + permutations) % M\n",
    "        \n",
    "        return pathway\n",
    "    \n",
    "    def evaluate(self, pathway, batch_size):\n",
    "        correct = 0\n",
    "        \n",
    "        for x, y in self.test_loader:\n",
    "            x, y = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "            \n",
    "            output = self.model(x, pathway)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            \n",
    "            correct += (pred == y.data).sum()\n",
    "            \n",
    "        accuracy = correct * 1.0 / len(self.test_loader) / batch_size\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def train_model(self, pathway):\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            # Stop training after 50 batches, evaluate fitness\n",
    "            if batch_idx >= self.batch_epochs:\n",
    "                fitness = self.evaluate(pathway, len(target))\n",
    "                return fitness\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = self.model(data, pathway)\n",
    "\n",
    "            loss = self.loss_func(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        fitnesses = []\n",
    "        best_pathway = None\n",
    "        best_fitness = -float('inf')\n",
    "        pathways = initialize_pathways()\n",
    "        gen = 0\n",
    "        \n",
    "        while best_fitness < self.convergence_threshold:\n",
    "            chosen_pathways = pathways[np.random.choice(P, 2)]\n",
    "            \n",
    "            current_fitnesses = []\n",
    "            \n",
    "            for pathway in chosen_pathways:\n",
    "                fitness = self.train_model(pathway)\n",
    "                \n",
    "                current_fitnesses.append(fitness)\n",
    "                \n",
    "                if fitness > best_fitness:\n",
    "                    best_fitness = fitness\n",
    "                    best_pathway = pathway\n",
    "                \n",
    "            # All pathways finished evaluating, copy the one with highest fitness\n",
    "            # to all other ones and mutate\n",
    "            pathways = np.array([best_pathway] + [self.mutate(deepcopy(best_pathway)) \n",
    "                                              for _ in range(P - 1)])\n",
    "            \n",
    "            fitnesses.append(max(current_fitnesses))\n",
    "            \n",
    "            if gen % 20 == 0:\n",
    "                print('Generation {} best fitness is {}'.format(gen, best_fitness))\n",
    "            gen += 1\n",
    "        \n",
    "        # Task training is done\n",
    "        self.model.done_task(best_pathway)\n",
    "        \n",
    "        return best_pathway, gen, fitnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classification between 8 and 1\n"
     ]
    }
   ],
   "source": [
    "# Configure high-level pipeline\n",
    "\n",
    "# Data loader\n",
    "task = BinaryClassificationTask()\n",
    "train_loader, test_loader = task.init(batch_size=16, noise_prob=0.5)\n",
    "\n",
    "# Model initialization\n",
    "task_layer = nn.Sequential(\n",
    "                    nn.Linear(20, 2), \n",
    "                    nn.Softmax())\n",
    "\n",
    "pathnet = PathNet(M)\n",
    "pathnet.initialize_new_task(task_layer)\n",
    "\n",
    "# Optimizer / loss\n",
    "optimizer_params = pathnet.get_optimizer_params()\n",
    "\n",
    "optimizer = optim.SGD(optimizer_params, lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0 best fitness is 0.537405303030303\n",
      "Generation 20 best fitness is 0.8129734848484849\n",
      "Generation 40 best fitness is 0.8129734848484849\n",
      "Generation 60 best fitness is 0.9597537878787878\n",
      "Generation 80 best fitness is 0.9668560606060606\n",
      "Generation 100 best fitness is 0.96875\n",
      "Generation 120 best fitness is 0.9725378787878788\n",
      "Generation 140 best fitness is 0.9753787878787878\n",
      "Generation 160 best fitness is 0.9753787878787878\n",
      "Generation 180 best fitness is 0.9753787878787878\n",
      "Generation 200 best fitness is 0.9753787878787878\n",
      "Generation 220 best fitness is 0.9753787878787878\n",
      "Generation 240 best fitness is 0.9763257575757576\n",
      "Generation 260 best fitness is 0.978219696969697\n",
      "Generation 280 best fitness is 0.978219696969697\n",
      "Generation 300 best fitness is 0.978219696969697\n",
      "Generation 320 best fitness is 0.978219696969697\n",
      "Generation 340 best fitness is 0.978219696969697\n",
      "Generation 360 best fitness is 0.978219696969697\n",
      "Generation 380 best fitness is 0.9805871212121212\n",
      "Generation 400 best fitness is 0.9805871212121212\n",
      "Generation 420 best fitness is 0.9805871212121212\n",
      "Generation 440 best fitness is 0.9805871212121212\n",
      "Generation 460 best fitness is 0.9805871212121212\n",
      "Generation 480 best fitness is 0.9805871212121212\n",
      "Generation 500 best fitness is 0.9805871212121212\n",
      "Generation 520 best fitness is 0.9805871212121212\n",
      "Generation 540 best fitness is 0.9805871212121212\n",
      "Generation 560 best fitness is 0.9805871212121212\n",
      "Generation 580 best fitness is 0.9805871212121212\n",
      "Generation 600 best fitness is 0.9805871212121212\n",
      "Generation 620 best fitness is 0.9805871212121212\n",
      "Generation 640 best fitness is 0.9805871212121212\n",
      "Generation 660 best fitness is 0.9805871212121212\n",
      "Generation 680 best fitness is 0.9805871212121212\n",
      "Generation 700 best fitness is 0.9805871212121212\n",
      "Generation 720 best fitness is 0.9805871212121212\n",
      "Generation 740 best fitness is 0.9805871212121212\n",
      "Generation 760 best fitness is 0.9805871212121212\n",
      "Generation 780 best fitness is 0.9805871212121212\n",
      "Generation 800 best fitness is 0.9805871212121212\n",
      "Generation 820 best fitness is 0.9805871212121212\n",
      "Generation 840 best fitness is 0.9805871212121212\n",
      "Generation 860 best fitness is 0.9805871212121212\n",
      "Generation 880 best fitness is 0.9805871212121212\n",
      "Generation 900 best fitness is 0.9805871212121212\n",
      "Generation 920 best fitness is 0.9805871212121212\n",
      "Generation 940 best fitness is 0.9805871212121212\n",
      "Generation 960 best fitness is 0.9805871212121212\n",
      "Generation 980 best fitness is 0.9805871212121212\n",
      "Generation 1000 best fitness is 0.9805871212121212\n",
      "Generation 1020 best fitness is 0.9805871212121212\n",
      "Generation 1040 best fitness is 0.9805871212121212\n",
      "Generation 1060 best fitness is 0.9805871212121212\n",
      "Generation 1080 best fitness is 0.9805871212121212\n",
      "Generation 1100 best fitness is 0.9805871212121212\n",
      "Generation 1120 best fitness is 0.9805871212121212\n",
      "Generation 1140 best fitness is 0.9805871212121212\n",
      "Generation 1160 best fitness is 0.9805871212121212\n",
      "Generation 1180 best fitness is 0.9805871212121212\n",
      "Generation 1200 best fitness is 0.9805871212121212\n",
      "Generation 1220 best fitness is 0.9805871212121212\n",
      "Generation 1240 best fitness is 0.9805871212121212\n",
      "Generation 1260 best fitness is 0.9805871212121212\n",
      "Generation 1280 best fitness is 0.9805871212121212\n",
      "Generation 1300 best fitness is 0.9805871212121212\n",
      "Generation 1320 best fitness is 0.9805871212121212\n",
      "Generation 1340 best fitness is 0.9805871212121212\n",
      "Generation 1360 best fitness is 0.9805871212121212\n",
      "Generation 1380 best fitness is 0.9805871212121212\n",
      "Generation 1400 best fitness is 0.9805871212121212\n",
      "Generation 1420 best fitness is 0.9805871212121212\n",
      "Generation 1440 best fitness is 0.9805871212121212\n",
      "Generation 1460 best fitness is 0.9805871212121212\n",
      "Generation 1480 best fitness is 0.9805871212121212\n"
     ]
    }
   ],
   "source": [
    "# PathNet Trainer\n",
    "evol_trainer = BinaryEvolutionTrainer(pathnet, optimizer, loss_func, \n",
    "                                      train_loader, test_loader, \n",
    "                                      convergence_threshold = 0.98)\n",
    "\n",
    "best_task_pathway, converge_generation, fitnesses = evol_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Model converge at generation', converge_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Re-write data pipeline + one-hot coding + flatten input  Checked\n",
    "# 2. Change loss function / output, change to Linear layers   Checked\n",
    "# 3. Transfer learning, save bestpath, freeze net, re-initialize net\n",
    "# 4. Train on second task, record transfer learning performance -> time vs accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_train_loader, new_test_loader = task.init(batch_size=16, noise_prob=0.5)\n",
    "\n",
    "# Model initialization\n",
    "new_task_layer = nn.Sequential(\n",
    "                    nn.Linear(20, 2), \n",
    "                    nn.Softmax())\n",
    "\n",
    "pathnet.initialize_new_task(new_task_layer)\n",
    "\n",
    "# Optimizer / loss\n",
    "optimizer_params = pathnet.get_optimizer_params()\n",
    "optimizer = optim.SGD(optimizer_params, lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_evol_trainer = BinaryEvolutionTrainer(pathnet, optimizer, loss_func, \n",
    "                                          new_train_loader, new_test_loader, \n",
    "                                          convergence_threshold = 0.98)\n",
    "new_best_task_pathway, new_converge_generation, new_fitnesses = new_evol_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Model converge at generation', new_converge_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(converge_generation)), fitnesses, c='r')\n",
    "plt.plot(list(range(new_converge_generation)), new_fitnesses, c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
